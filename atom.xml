<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Pedro Jiménez's Blog]]></title>
  <link href="http://pedrojimenez.github.com/atom.xml" rel="self"/>
  <link href="http://pedrojimenez.github.com/"/>
  <updated>2012-12-11T16:14:55+01:00</updated>
  <id>http://pedrojimenez.github.com/</id>
  <author>
    <name><![CDATA[Pedro Jiménez Solís]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Chef: Bootstrapping for first time]]></title>
    <link href="http://pedrojimenez.github.com/blog/2012/12/11/chef-bootstrapping-for-first-time/"/>
    <updated>2012-12-11T15:36:00+01:00</updated>
    <id>http://pedrojimenez.github.com/blog/2012/12/11/chef-bootstrapping-for-first-time</id>
    <content type="html"><![CDATA[<center>
<img align=center src="http://docs.opscode.com/_static/opscode_chef_html_logo.jpg">
</center>


<p>Hemos arrancado por fin nuestro laboratorio completamente con Chef y nos tocaba probar uno de los trucos/habilidades más asombrosas de Knife, BooTstrap.</p>

<p>Gracias a este comando podremos incluir a un nodo que no estuviera dado de alta en nuestra infraestructura desde alguno de los nodos de &#8220;administración&#8221;, completando su registro e instalando el software de Chef en dicho nodo. Además crea el fichero de configuración &#8220;client.rb&#8221; y los dos ficheros de claves, el de cliente (client.pem) y el del servidor (validation.pem) en el nodo.</p>

<pre>
[ coruscant:~ ] knife bootstrap 192.168.1.242 -x operador -P onetimepassword --sudo
Bootstrapping Chef on 192.168.1.242
192.168.1.242 [Tue, 11 Dec 2012 15:14:48 +0100] INFO: *** Chef 10.14.4 ***
192.168.1.242 [Tue, 11 Dec 2012 15:14:49 +0100] INFO: Client key /etc/chef/client.pem is not present - registering
192.168.1.242 [Tue, 11 Dec 2012 15:14:49 +0100] INFO: HTTP Request Returned 404 Not Found: Cannot load node compute02
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] INFO: Setting the run_list to [] from JSON
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] INFO: Run List is []
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] INFO: Run List expands to []
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] INFO: Starting Chef Run for compute02
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] INFO: Running start handlers
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] INFO: Start handlers complete.
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] INFO: Loading cookbooks []
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] WARN: Node compute02 has an empty run list.
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] INFO: Chef Run complete in 0.632681 seconds
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] INFO: Running report handlers
192.168.1.242 [Tue, 11 Dec 2012 15:14:50 +0100] INFO: Report handlers complete
</pre>




<!-- more -->


<p>A partir de este momento desde el propio Chef Server se introduce el nodo en el entorno deseado (Folsom en nuestro caso) y se le añaden los &#8220;Roles&#8221; que vaya a desempeñar.</p>

<blockquote><p>Se le podría pasar un archivo JSON (con la opción <strong>-j</strong>) para pasarle directamente el Entorno y los roles que se le van a aplicar. Dejo esta parte pendiente para un post posterior
que rellenaré un poco más adelante y cerrar así el círculo de la instalación distribuida con Chef/Knife.</p></blockquote>

<p>NOTA: Si existe el fichero de claves del cliente en la máquina destino , se generará un error</p>

<pre>
[ coruscant:~ ] knife bootstrap 192.168.1.242 -x operador -P operador --sudo
Bootstrapping Chef on 192.168.1.242
192.168.1.242 [Tue, 11 Dec 2012 15:07:23 +0100] INFO: *** Chef 10.14.4 ***
192.168.1.242 [Tue, 11 Dec 2012 15:07:23 +0100] INFO: HTTP Request Returned 401 Unauthorized: Failed to authenticate. Ensure that your client key is valid.
192.168.1.242 
192.168.1.242 ================================================================================
192.168.1.242 Chef encountered an error attempting to load the node data for "compute02"
192.168.1.242 ================================================================================
192.168.1.242 
192.168.1.242 Authentication Error:
192.168.1.242 ---------------------
192.168.1.242 Failed to authenticate to the chef server (http 401).
192.168.1.242 
192.168.1.242 Server Response:
192.168.1.242 ----------------
192.168.1.242 Failed to authenticate. Ensure that your client key is valid.
192.168.1.242 
192.168.1.242 Relevant Config Settings:
192.168.1.242 -------------------------
192.168.1.242 chef_server_url   "http://172.16.0.51:4000"
192.168.1.242 node_name         "compute02"
192.168.1.242 client_key        "/etc/chef/client.pem"
192.168.1.242 
192.168.1.242 If these settings are correct, your client_key may be invalid.
192.168.1.242 
192.168.1.242 [Tue, 11 Dec 2012 15:07:23 +0100] FATAL: Stacktrace dumped to /var/chef/cache/chef-stacktrace.out
192.168.1.242 [Tue, 11 Dec 2012 15:07:23 +0100] FATAL: Net::HTTPServerException: 401 "Unauthorized"
</pre>


<p>Fuente: <a href="http://wiki.opscode.com/display/chef/Knife+Bootstrap">http://wiki.opscode.com/display/chef/Knife+Bootstrap</a>
Fuente Alternativa: <strong>@Sfrek</strong> y <strong>@achilued</strong> (gracias por toda vuestra ayuda y paciencia)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cinder: Volume Creation Error in Folsom]]></title>
    <link href="http://pedrojimenez.github.com/blog/2012/12/05/cinder-volume-creation-error-in-folsom/"/>
    <updated>2012-12-05T17:45:00+01:00</updated>
    <id>http://pedrojimenez.github.com/blog/2012/12/05/cinder-volume-creation-error-in-folsom</id>
    <content type="html"><![CDATA[<center> 
    <img src="http://wiki.openstack.org/Icons?action=AttachFile&do=get&target=openstack-object-storage-icon.png" alt="Cinder">
    <img src="http://wiki.openstack.org/Icons?action=AttachFile&do=get&target=openstack-object-storage-icon.png" alt="Cinder">
    <img src="http://wiki.openstack.org/Icons?action=AttachFile&do=get&target=openstack-object-storage-icon.png" alt="Cinder">
</center>


<br /><br />


<p>Estamos inmersos en la automatización de las instalaciones de Openstack con Chef. Utilizamos actualmente una infraestructura muy sencilla con un Chef-Server local que vamos utilizando para diferentes clientes/laboratorios.</p>

<p>Una de las varias modificaciones que hemos tenido que realizar ha sido con el cookbook de Cinder. Nos hemos encontrado que después de tocar atributos en el &#8220;Rol&#8221; y el &#8220;Entorno&#8221; nos seguía dando error al crear el volumen desde consola. Más específicamente un error de iSCSI:</p>

<pre>
2012-12-05 17:01:19 4046 ERROR cinder.openstack.common.rpc.amqp [-] Exception during message handling
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp   File "/usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py", line 276, in _process_data
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp     rval = self.proxy.dispatch(ctxt, version, method, **args)
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp   File "/usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/dispatcher.py", line 145, in dispatch
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp   File "/usr/lib/python2.7/dist-packages/cinder/volume/manager.py", line 163, in create_volume
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp     volume_ref['id'], {'status': 'error'})
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp   File "/usr/lib/python2.7/contextlib.py", line 24, in __exit__
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp   File "/usr/lib/python2.7/dist-packages/cinder/volume/manager.py", line 156, in create_volume
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp     model_update = self.driver.create_export(context, volume_ref)
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp   File "/usr/lib/python2.7/dist-packages/cinder/volume/driver.py", line 437, in create_export
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp     volume_path)
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp   File "/usr/lib/python2.7/dist-packages/cinder/volume/iscsi.py", line 145, in create_iscsi_target
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp     raise exception.ISCSITargetCreateFailed(volume_id=vol_id)
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp ISCSITargetCreateFailed: Failed to create iscsi target for volume volume-4e680c7b-b8f4-43b8-a766-1996a2537474.
2012-12-05 17:01:19 4046 TRACE cinder.openstack.common.rpc.amqp 
</pre>


<!-- more -->


<p>Buscando algo de información nos hemos topado con un pequeño <em>BUG</em>, muy sencillo, ya que es un error de interpretación de la sintaxis de un archivo de configuración (<em>/etc/tgt/targets.conf</em>). En él se indica con un comodín que añadan todas las opciones de configuración en archivos del directorio <em>/etc/tgt/conf.d/</em> &#8230; aunque parece que esta sintaxis no funciona:</p>

<pre>
ARCHIVO: /etc/tgt/targets.conf

Cambiamos esto:
include /etc/tgt/conf.d/*.conf
default-driver iscsi


Por los archivos que queramos incluir:
include /etc/tgt/conf.d/cinder_tgt.conf
include /etc/tgt/conf.d/nova_tgt.conf
default-driver iscsi
</pre>


<p>Realizamos un reinicio del servicio y volvemos a crear un volumen (Volume-Test03)</p>

<pre>
root@controller:~# cinder create --display-name LV_test03 --display-description "Testing Volume 03" 2
+---------------------+--------------------------------------+
|       Property      |                Value                 |
+---------------------+--------------------------------------+
|     attachments     |                  []                  |
|  availability_zone  |                 nova                 |
|      created_at     |      2012-12-05T16:33:35.452347      |
| display_description |          Testing Volume 03           |
|     display_name    |              LV_test03               |
|          id         | 6553512d-c25a-40a2-9a50-bb12d35b23bf |
|       metadata      |                  {}                  |
|         size        |                  2                   |
|     snapshot_id     |                 None                 |
|        status       |               creating               |
|     volume_type     |                 None                 |
+---------------------+--------------------------------------+
</pre>


<p>Comprobamos que ahora el volumen se ha creado de manera correcta</p>

<pre>
root@controller:~# cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| 6553512d-c25a-40a2-9a50-bb12d35b23bf | available |  LV_test03   |  2   |     None    |          |             |
| df6949a3-beab-481c-ad43-3e6961dd1266 |   error   |  LV_test02   |  2   |     None    |          |             |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+

LOG:
==> /var/log/cinder/cinder-api.log <==
2012-12-05 17:33:35 INFO cinder.api.openstack.wsgi [req-b65efc22-c141-4d7d-8b55-67d5e01ca6ca 0b4cf20489b74be5955824ea9df14d4d 13b18da27978481f893cbdd938d8d78f] POST http://172.16.172.4:8776/v1/13b18da27978481f893cbdd938d8d78f/volumes
2012-12-05 17:33:35 AUDIT cinder.api.openstack.volume.volumes [req-b65efc22-c141-4d7d-8b55-67d5e01ca6ca 0b4cf20489b74be5955824ea9df14d4d 13b18da27978481f893cbdd938d8d78f] Create volume of 2 GB
2012-12-05 17:33:35 DEBUG cinder.quota [req-b65efc22-c141-4d7d-8b55-67d5e01ca6ca 0b4cf20489b74be5955824ea9df14d4d 13b18da27978481f893cbdd938d8d78f] Created reservations ['3c601bff-5b57-440c-88fd-f81c54d47570', '060b181f-1ee5-4d0d-99da-985d5c094d34'] reserve /usr/lib/python2.7/dist-packages/cinder/quota.py:663
2012-12-05 17:33:35 4604 DEBUG cinder.openstack.common.rpc.amqp [-] Making asynchronous cast on cinder-scheduler... cast /usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py:377
</pre>


<blockquote><p>   <em>NOTA</em>: Debe usarse un archivo &#8220;openrc&#8221;/&#8221;novarc&#8221; para cargar las variables de entorno y no tener que especificarslas a Cinder.</p>

<p>   <em>PROBLEMA</em>: en cada iteración de &#8220;Chef client&#8221; el archivo se regerará al valor erróneo. Cambiamos la plantilla para que adapte a las especificaciones arriba mencionadas.</p></blockquote>

<p> ARCHIVO: cinder/templates/default/targets.conf.erb</p>

<p> Cambiamos:</p>

<pre class=ruby>
 <% if %w{redhat centos fedora}.include?(node["platform"]) %>
 include /var/lib/cinder/volumes/*
 <% end %>
 <% if %w{debian ubuntu}.include?(node["platform"]) %>
 include /etc/tgt/conf.d/*.conf
 <% end %>
 default-driver iscsi
</pre>


<p> Por:</p>

<pre class=ruby>
 <% if %w{redhat centos fedora}.include?(node["platform"]) %>
 include /var/lib/cinder/volumes/*
 <% end %>
 <% if %w{debian ubuntu}.include?(node["platform"]) %>
 include /etc/tgt/conf.d/cinder_tgt.conf
 include /etc/tgt/conf.d/nova_tgt.conf
 <% end %>
 default-driver iscsi
</pre>


<p>Debo dar la gracias a la gente que ha estado siguiendo esto en Launchpad: <a href="https://bugs.launchpad.net/cinder/+bug/1057904">https://bugs.launchpad.net/cinder/+bug/1057904</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Database Access Error in Folsom]]></title>
    <link href="http://pedrojimenez.github.com/blog/2012/11/16/database-access-error-in-folsom/"/>
    <updated>2012-11-16T11:32:00+01:00</updated>
    <id>http://pedrojimenez.github.com/blog/2012/11/16/database-access-error-in-folsom</id>
    <content type="html"><![CDATA[<p>Después de darle muchas vueltas al asunto, el comodín que se utilizaba en Mysql 5.1 (versión usada Diablo and Essex) el conocido &#8220;%&#8221; ha comenzado a fallar. Nos encontramos con errores de acceso a las Bases de Datos que eran nuevos para mi hasta el momento. Revisando las bitácoras de instalaciones previas hemos visto que los permisos de acceso a las diferentes DataBases de Mysql han cambiado y se utiliza como única línea de permisos la siguiente:</p>

<pre>
mysql_database_user[keystone]: granting access with statement [GRANT all ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'onetimepassword']
</pre>


<p>Pero solamente con esta sentencia constatamos que el acceso desde otros nodos (multihost) nos daba error. Para solucionar estos problemas se ha probado a añadir sentencias extras de &#8220;GRANT&#8221; hasta que permitió dichas conexiones. Haciendo un resumen de todas ellas se ha modificado el cookbook de osops-utils para que añada una nueva sentencia de GRANT contra la dirección en la que se bindea el servicio de Mysql. De esta manera se podrá configurar el servicio de Base de Datos en HA y tener los accesos contra la &#8220;bind_address&#8221;.</p>

<pre>
Archivo "osops-utils/libraries/database.rb"

        mysql_database_user username do
          connection connection_info
          password pw
          database_name db_name
          host "#{mysql_info["bind_address"]}"
          privileges [:all]
          action :grant
        end
</pre>


<p>Se ha hecho un <em>Pull Request</em> a la gente de <em>Rcbops</em> con esta modificación.<a href="https://github.com/rcbops-cookbooks/osops-utils/pull/46#issuecomment-10194833">Enlace</a>.</p>

<p>Ahora mismo lo estoy usando en nuestro Chef Server en el despligue de Folsom y funciona &#8220;like a charm&#8221;. Nos genera dos sentencias de permisos para cada servicio (keystone/glance/nova/horizon).</p>

<pre>
INFO: Processing mysql_database_user[keystone] action grant (keystone::server line 24)
INFO: mysql_database_user[keystone]: granting access with statement [GRANT all ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'onetimepassword']
INFO: Processing mysql_database_user[keystone] action grant (keystone::server line 32)
INFO: mysql_database_user[keystone]: granting access with statement [GRANT all ON keystone.* TO 'keystone'@'172.16.172.4' IDENTIFIED BY 'onetimepassword']
</pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Keystone user-role-list bug]]></title>
    <link href="http://pedrojimenez.github.com/blog/2012/11/15/keystone-user-role-list-bug/"/>
    <updated>2012-11-15T15:02:00+01:00</updated>
    <id>http://pedrojimenez.github.com/blog/2012/11/15/keystone-user-role-list-bug</id>
    <content type="html"><![CDATA[<p>Depurando el comportamiento de Keystone para solventar un error que me trae de cabeza en Glance, he visto que un nuevos comandos que se han añadido al servicio de Identidad en OpenStack (&#8220;keystone user-role-list&#8221;). Este comando nos permitirá extraer un listado de los usuarios/roles/tenants de un solo vistazo.</p>

<p>Problema: que ahora mismo no funciona como se espera. Tan solo muestra el listado de los roles que hay para el usurio que tenemos en las variables de entorno (cargadas en el fichero novarc o como queramos llamarlo). Necesitaba conocer dicho listado del usuario &#8220;glance&#8221; no del usuario &#8220;admin&#8221;. En una primera pasada vemos que el listado solamente nos permite obtener los del mismo usuario &#8220;admin&#8221;:</p>

<pre>
keystone role-list
+----------------------------------+----------------------+
|                id                |         name         |
+----------------------------------+----------------------+
| 06bc817f9b8647d3bb795c6a1107cff3 |    KeystoneAdmin     |
| 1e77a4036d42446aabcfc706759b732d | KeystoneServiceAdmin |
| 25fcba9f9b9a4982a77f70fd2c84fc31 |        admin         |
| 28941e6016ea488cad9e5e55fee7d8aa |        Member        |
+----------------------------------+----------------------+

keystone tenant-list
+----------------------------------+---------+---------+
|                id                |   name  | enabled |
+----------------------------------+---------+---------+
| 15eafea37ce24b42ab60b1fe0f882d98 | service |   true  |
| 18e181671f174cc1bce64e4370eea96a |  admin  |   true  |
+----------------------------------+---------+---------+

keystone user-list
+----------------------------------+------------+---------+-------+
|                id                |    name    | enabled | email |
+----------------------------------+------------+---------+-------+
| 4be33d2e90ad42ce867954b07aa7b908 |   admin    |   true  |       |
| 8c53cdc1f4354e77a8e2abd39728b20d |    nova    |   true  |       |
| bd9e78ff6ca84c7181f3f412a01597ed | monitoring |   true  |       |
| f6b72f21414246d0a924fc883e694f45 |   glance   |   true  |       |
+----------------------------------+------------+---------+-------+

keystone user-role-list
+----------------------------------+----------------------+----------------------------------+----------------------------------+
|                id                |         name         |             user_id              |            tenant_id             |
+----------------------------------+----------------------+----------------------------------+----------------------------------+
| 06bc817f9b8647d3bb795c6a1107cff3 |    KeystoneAdmin     | 4be33d2e90ad42ce867954b07aa7b908 | 18e181671f174cc1bce64e4370eea96a |
| 1e77a4036d42446aabcfc706759b732d | KeystoneServiceAdmin | 4be33d2e90ad42ce867954b07aa7b908 | 18e181671f174cc1bce64e4370eea96a |
| 25fcba9f9b9a4982a77f70fd2c84fc31 |        admin         | 4be33d2e90ad42ce867954b07aa7b908 | 18e181671f174cc1bce64e4370eea96a |
+----------------------------------+----------------------+----------------------------------+----------------------------------+
</pre>


<p>Para este listado el usuario sigue siendo el mismo: 4be33d2e90ad42ce867954b07aa7b908 y por un ratillo incluso pensé que el resto de usuarios no se habrían creado, pero no era realmente así. Un pequeño vistazo a la DB me lo dejó bien claro. Buscando algo de información sobre el comando (maravilla de doc oficial) y al hacer el &#8220;help&#8221; del comando me encontré que se puede especificar el usuario y el tenant.</p>

<p>Pues manos a la obra, ahora es viable sacar el resto de la información para completar lo que en un futuro será la salida estandar del comando. Tampoco estaría mal poder especificar uno de los dos parámetros, o bien el usuario o bien el tenant, pero os adelanto que no es así. Hay que especificar ambos.</p>

<pre>
keystone help user-role-list
usage: keystone user-role-list [--user-id <user-id>] [--tenant-id <tenant-id>]

List roles granted to a user

Optional arguments:
  --user-id <user-id>   List roles granted to a user
  --tenant-id <tenant-id>
                        List roles granted on a tenant

- - - - - - - - - - -
user Glance:
root@controller:/etc/glance# keystone user-role-list --user-id f6b72f21414246d0a924fc883e694f45 --tenant-id 15eafea37ce24b42ab60b1fe0f882d98
+----------------------------------+-------+----------------------------------+----------------------------------+
|                id                |  name |             user_id              |            tenant_id             |
+----------------------------------+-------+----------------------------------+----------------------------------+
| 25fcba9f9b9a4982a77f70fd2c84fc31 | admin | f6b72f21414246d0a924fc883e694f45 | 15eafea37ce24b42ab60b1fe0f882d98 |
+----------------------------------+-------+----------------------------------+----------------------------------+

user Nova:
root@controller:/etc/glance# keystone user-role-list  --user-id 8c53cdc1f4354e77a8e2abd39728b20d --tenant-id 15eafea37ce24b42ab60b1fe0f882d98
+----------------------------------+-------+----------------------------------+----------------------------------+
|                id                |  name |             user_id              |            tenant_id             |
+----------------------------------+-------+----------------------------------+----------------------------------+
| 25fcba9f9b9a4982a77f70fd2c84fc31 | admin | 8c53cdc1f4354e77a8e2abd39728b20d | 15eafea37ce24b42ab60b1fe0f882d98 |
+----------------------------------+-------+----------------------------------+----------------------------------+


Rol Admin: | 25fcba9f9b9a4982a77f70fd2c84fc31 |
</pre>


<p>Espero que os sirva para un apuro o para el debug de vuestra plataforma.</p>

<p>Enlaces:
<u>http://docs.openstack.org/trunk/openstack-compute/admin/content/adding-users-tenants-and-roles-with-python-keystoneclient.html</u><br />
<u>https://bugs.launchpad.net/python-keystoneclient/+bug/1058750</u><br />
<u>http://www.gossamer-threads.com/lists/openstack/dev/19510</u><br /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu Keyring password and Folsom]]></title>
    <link href="http://pedrojimenez.github.com/blog/2012/11/15/ubuntu-keyring-password-and-folsom/"/>
    <updated>2012-11-15T11:16:00+01:00</updated>
    <id>http://pedrojimenez.github.com/blog/2012/11/15/ubuntu-keyring-password-and-folsom</id>
    <content type="html"><![CDATA[<p>Pues después de varios intentos con la infraestructura de Folsom, al final en todas ellas he conseguido llegar a este punto de error. Después de leer por ahí, la solución es bastante sencilla y se aplica de manea instantánea sobre línea de comandos.</p>

<p>Este error nos salta cuando tratamos de invocar los comandos de &#8220;nova&#8221; después de haber generado y cargado las variables de entorno en el fichero novarc (o el nombre que elijamos).</p>

<pre>
root@controller:~# nova list
Please set a password for your new keyring
</pre>


<p>Aunque metamos una contraseña, nos seguirá molestando con este error de manera continuada. Para evitarlo basta con añadir la línea de abajo al bashrc del usuario y volver a cargarlo.</p>

<pre>
Añadimos al /home/user/.bashrc y recargamos
export OS_NO_CACHE=1
source /home/user/.bashrc 
</pre>


<p>Comprobamos que se ejecutan los comandos de &#8220;nova&#8221;:</p>

<pre>
root@controller:/etc/nova# nova volume-list
+--------------------------------------+-----------+-----------------+------+-------------+-------------+
| ID                                   | Status    | Display Name    | Size | Volume Type | Attached to |
+--------------------------------------+-----------+-----------------+------+-------------+-------------+
| 850e5d38-dc87-4267-b3c9-9d02205f11ab | available | MiPrimerVolumen | 3    | None        |             |
+--------------------------------------+-----------+-----------------+------+-------------+-------------+
</pre>


<p>La segunda opción es ejecutar los comandos de &#8220;nova&#8221; con la opción <u>&#8211;no_cache</u>, aunque sobre el mismo entorno no he sido capaz de que funcione con dichas indicaciones.</p>

<pre>
root@controller:~# nova --no_cache volume-list
</pre>


<p>Enlaces:</p>

<p><u>https://bugs.launchpad.net/python-novaclient/+bug/1020238</u></p>

<p><u>https://lists.launchpad.net/openstack/msg16095.html (solución de Vish)</u></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Primeros Pasos]]></title>
    <link href="http://pedrojimenez.github.com/blog/2012/10/31/primeros-pasos/"/>
    <updated>2012-10-31T12:03:00+01:00</updated>
    <id>http://pedrojimenez.github.com/blog/2012/10/31/primeros-pasos</id>
    <content type="html"><![CDATA[<p>Bueno por fin me he decidido a escribir algo de todo lo que hago y parte de lo que leo. He valorado un montón de opciones previas pero creo que Octopress se adapta perfectamente a mis necesidades. Es por supuesto software libre y como ya hemos visto desplegable dentro de una infraestructura PaaS.</p>

<p>Gracias a @achilued y a @Sfrek por el apoyo y los ánimos. Espero darle la vidilla que se merece ;)</p>
]]></content>
  </entry>
  
</feed>
